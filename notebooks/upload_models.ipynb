{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload experimental models to Hugging Face Model repositories\n",
    "This notebook is a helper for uploading pre-trained models to Hugging Face. It allows you to add README info for experiments at upload time for better documentation. \n",
    "\n",
    "*First*: Make sure that you have added your HuggingFace Hub token in some way or logged in on the command line via `huggingface-cli login`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub.utils import HfHubHTTPError\n",
    "import transformers\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name prefix to \n",
    "MODEL_ROOT = Path(\"../data/models/\")\n",
    "ALL_MODELS_README = \"\"\"\n",
    "---\n",
    "license: mit\n",
    "language:\n",
    "- en\n",
    "pipeline_tag: automatic-speech-recognition\n",
    "---\n",
    "# About \n",
    "This model was created to support experiments for evaluating phonetic transcription \n",
    "with the Buckeye corpus as part of https://github.com/ginic/multipa. \n",
    "This is a version of facebook/wav2vec2-large-xlsr-53 fine tuned on a specific subset of the Buckeye corpus.\n",
    "For details about specific model parameters, please view the config.json here or \n",
    "training scripts in the scripts/buckeye_experiments folder of the GitHub repository. \n",
    "\n",
    "# Experiment Details\n",
    "\"\"\"\n",
    "\n",
    "TIMIT_FINED_TUNED_README =\"\"\"\n",
    "---\n",
    "license: mit\n",
    "language:\n",
    "- en\n",
    "pipeline_tag: automatic-speech-recognition\n",
    "---\n",
    "# About \n",
    "This model was created to support experiments for evaluating phonetic transcription \n",
    "with the Buckeye and TIMIT corpus as part of https://github.com/ginic/multipa. \n",
    "This is a version of excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k that was further fine-tuned on a subset of the Buckeye corpus.\n",
    "For details about specific model parameters, please view the config.json here or \n",
    "training scripts in the scripts/fine_tuning_experiments folder of the GitHub repository. \n",
    "\n",
    "# Experiment Details\n",
    "\"\"\"\n",
    "\n",
    "# Specific sets of experiments have more details. I just copied these from the EXPERIMENT_LOG.md \n",
    "README_MAPPINGS = {\n",
    "#     # This was the best hyperparam tuned model & these model parameters were used for all other experiments\n",
    "#     \"hyperparam_tuning_1\":\"\"\"The best performing model from hyperparameter tuning experiments (batch size, learning rat, base model to fine tune). Vary the random seed to select training data while keeping an even 50/50 gender split to measure statistical significance of changing training data selection. Retrain with the same model parameters, but different data seeding to measure statistical significance of data seed, keeping 50/50 gender split. \n",
    "\n",
    "# Goals: \n",
    "# - Choose initial hyperparameters (batch size, learning rat, base model to fine tune) based on validation set performance\n",
    "# - Establish whether data variation with the same gender makeup is statistically significant in changing performance on the test set (first data_seed experiment)\n",
    "# \"\"\",\n",
    "#     \"data_seed_bs64\": \"\"\"Vary the random seed to select training data while keeping an even 50/50 gender split to measure statistical significance of changing training data selection. Retrain with the same model parameters, but different data seeding to measure statistical significance of data seed, keeping 50/50 gender split. \n",
    "\n",
    "# Goals: \n",
    "# - Establish whether data variation with the same gender makeup is statistically significant in changing performance on the test set\n",
    "\n",
    "# Params to vary:\n",
    "# - training data seed (--train_seed): [91, 114, 771, 503]\n",
    "# \"\"\",\n",
    "\n",
    "#     \"gender_split\": \"\"\"Still training with a total amount of data equal to half the full training data (4000 examples), vary the gender split 30/70, but draw examples from all individuals. Do 5 models for each gender split with the same model parameters but different data seeds. \n",
    "\n",
    "# Goals: \n",
    "# - Determine how different in gender split in training data affects performance\n",
    "\n",
    "# Params to vary: \n",
    "# - percent female (--percent_female) [0.0, 0.3, 0.7, 1.0]\n",
    "# - training seed (--train_seed)\n",
    "# \"\"\", \n",
    "\n",
    "#     \"vary_individuals\": \"\"\"These experiments keep the total amount of data equal to half the training data with the gender split 50/50, but further exclude certain speakers completely using the --speaker_restriction argument. This allows us to restrict speakers included in training data in any way. For the purposes of these experiments, we are focussed on the age demogrpahic of the user.  \n",
    "\n",
    "# For reference, the speakers and their demographics included in the training data are as follows where the speaker age range 'y' means under 30 and 'o' means over 40: \n",
    "\n",
    "# | speaker_id | speaker_gender | speaker_age_range | \n",
    "# | ---------- | -------------- | ----------------- |\n",
    "# | S01 | f | y |\n",
    "# | S04 | f | y | \n",
    "# | S08 | f | y | \n",
    "# | S09 | f | y | \n",
    "# | S12 | f | y | \n",
    "# | S21 | f | y | \n",
    "# | S02 | f | o |\n",
    "# | S05 | f | o | \n",
    "# | S07 | f | o | \n",
    "# | S14 | f | o | \n",
    "# | S16 | f | o |\n",
    "# | S17 | f | o | \n",
    "# | S06 | m | y | \n",
    "# | S11 | m | y | \n",
    "# | S13 | m | y | \n",
    "# | S15 | m | y | \n",
    "# | S28 | m | y | \n",
    "# | S30 | m | y |\n",
    "# | S03 | m | o | \n",
    "# | S10 | m | o | \n",
    "# | S19 | m | o |\n",
    "# | S22 | m | o |\n",
    "# | S24 | m | o | \n",
    "\n",
    "\n",
    "# Goals: \n",
    "# - Determine how variety of speakers in the training data affects performance\n",
    "\n",
    "# Params to vary: \n",
    "# - training seed (--train_seed)\n",
    "# - demographic make up of training data by age, using --speaker_restriction \n",
    "#     - Experiments `young_only`: only individuals under 30, S01 S04 S08 S09 S12 S21 S06 S11 S13 S15 S28 S30\n",
    "#     - Experiments `old_only`: only individuals over 40, S02 S05 S07 S14 S16 S17 S03 S10 S19 S22 S24\n",
    "# \"\"\"\n",
    "#     \"full_dataset\": \"\"\"The entire train split of the Buckeye corpus was used to train this model. \n",
    "# The only data excluded are samples in the train split that are too short (< 0.1 seconds) or too long (>12 seconds) to be used to train the model \n",
    "\n",
    "# Goals: \n",
    "# - Include the largest amount of training data possible. \n",
    "# - Can be used with a different corpus (e.g. TIMIT, Speech Accent Archive) for evaluation to test generalization to other dialects and language varieties. \n",
    "# \"\"\"\n",
    "\n",
    "    \"fine_tune_data_seed\": \"\"\"These experiments take a wav2vec2.0 model originially fine-tuned on TIMIT (excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k) and further fine-tune it on the Buckeye corpus.\n",
    "The random seed is varied to select training data while keeping an even 50/50 gender split to measure significance of changing training data selection.\n",
    "\n",
    "Goals:\n",
    "- Determine how additional fine-tuning on different corpora affect performance on test sets for both corpora\n",
    "- Establish whether data variation with the same gender makeup is statistically significant in changing performance on the test set\n",
    "\n",
    "Params to vary:\n",
    "- training data seed (--train_seed)\n",
    "- batch size: [64, 32] will be indicated at the end of the model name following \"bs\"\n",
    "\"\"\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ../data/models/fine_tune_data_seed_bs32_1 matches prefix 'fine_tune_data_seed'.\n",
      "Uploading to hub as: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044288a46bfe4073885ced88faf89e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d61dfb655b94a67b8b019e8554d9b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91a96a65ead54852affe4cbebb75407d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpid9b1obf/model.safetensors    :   1%|1         | 16.8MB / 1.26GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading README for ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_1\n",
      "Model ../data/models/fine_tune_data_seed_bs32_2 matches prefix 'fine_tune_data_seed'.\n",
      "Uploading to hub as: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "334cf080ecfa46a7b5e1ef839a03fdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1d6cfcfebf424aa68455d43ef0103e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9f3cee0e76c45d6ac9417e7b8e35dbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmp_jssw771/model.safetensors    :   2%|1         | 25.1MB / 1.26GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading README for ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_2\n",
      "Model ../data/models/fine_tune_data_seed_bs32_3 matches prefix 'fine_tune_data_seed'.\n",
      "Uploading to hub as: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c186fc4d3924eb6b197ac51ff84fad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91431030ed94d2ea78616d1616402e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165ca99a789e4a6fa309514fa4588a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpv1ydz2ho/model.safetensors    :   3%|2         | 33.5MB / 1.26GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading README for ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3\n",
      "Model ../data/models/fine_tune_data_seed_bs32_4 matches prefix 'fine_tune_data_seed'.\n",
      "Uploading to hub as: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e7817cfc8c446d88ccf8df768d5124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf851a1103604400be672cc5a8ac4e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c109dddadb41451a8a004d1b30f0fbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpzybzlrov/model.safetensors    :   2%|1         | 25.1MB / 1.26GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading README for ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_4\n",
      "Model ../data/models/fine_tune_data_seed_bs32_5 matches prefix 'fine_tune_data_seed'.\n",
      "Uploading to hub as: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202d1622f0bc4c17b7fe98be6c70a234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e7f76e68534ed1a7cee0d3cc912f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33f24f222e8b4cc5af96f6a77cbbeefa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpx93xvgsw/model.safetensors    :   1%|          | 9.75MB / 1.26GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading README for ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_5\n",
      "Model ../data/models/fine_tune_data_seed_bs64_1 matches prefix 'fine_tune_data_seed'.\n",
      "Uploading to hub as: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs64_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28c72ef60154a7ba5c3acc2de1b319e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd288db47324502a6a16fc1a128eea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fffb2f8def423a880ecf8a5e64106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpnp_mkgd8/model.safetensors    :   3%|2         | 33.4MB / 1.26GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading README for ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs64_1\n",
      "Model ../data/models/fine_tune_data_seed_bs64_2 matches prefix 'fine_tune_data_seed'.\n",
      "Uploading to hub as: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs64_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fdd7e7f0314f9cac8575458af50a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22468eddc324ad9a6344afbc68ed30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "018c1b2e732b4b3d9607705fdc674196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpglqckrt9/model.safetensors    :   0%|          | 3.79MB / 1.26GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading README for ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs64_2\n",
      "Model ../data/models/fine_tune_data_seed_bs64_3 matches prefix 'fine_tune_data_seed'.\n",
      "Uploading to hub as: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs64_3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16276e6b8d764c0c9f5a54c1bdfc1a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2776df695a4f89a7ef015eb7468cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4833398d33fe4bba86432d22115faed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpwvei7d7s/model.safetensors    :   1%|          | 7.58MB / 1.26GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading README for ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs64_3\n",
      "Model ../data/models/fine_tune_data_seed_bs64_4 matches prefix 'fine_tune_data_seed'.\n",
      "Uploading to hub as: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs64_4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2cad2cf2be9433e8fc7fe46a35b2a78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d222f62731e14d3399336b3ca2863115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca6b6c0d663448b5babb4b8269dedfa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpm0kfs67p/model.safetensors    :   1%|          | 9.19MB / 1.26GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading README for ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs64_4\n",
      "Model ../data/models/fine_tune_data_seed_bs64_5 matches prefix 'fine_tune_data_seed'.\n",
      "Uploading to hub as: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs64_5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a277cc763f0a4054bcb5bd89ad07ca04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)                : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515f6e140f4942f7b5d41353768d3911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload                         : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e701d0253e4c48a887cec31359ae80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  /tmp/tmpxmna6bm8/model.safetensors    :   1%|          | 7.05MB / 1.26GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading README for ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs64_5\n"
     ]
    }
   ],
   "source": [
    "api = HfApi()\n",
    "for model_folder in MODEL_ROOT.iterdir():\n",
    "    if model_folder.is_dir(): \n",
    "        for prefix in README_MAPPINGS.keys(): \n",
    "            if model_folder.name.startswith(prefix):\n",
    "                print(f\"Model {model_folder} matches prefix '{prefix}'.\")\n",
    "                if prefix==\"fine_tune_data_seed\":\n",
    "                    hub_name = f\"ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_{model_folder.name[-6:]}\"\n",
    "                    full_readme = \"\".join([TIMIT_FINED_TUNED_README, README_MAPPINGS[prefix]])\n",
    "                    model_to_upload = model_folder / \"wav2vec2-large-lv60_phoneme-timit_english_timit-4k-buckeye-ipa\"\n",
    "                else:\n",
    "                \n",
    "                    hub_name = f\"ginic/{model_folder.name}_wav2vec2-large-xlsr-53-buckeye-ipa\" \n",
    "            \n",
    "                    full_readme = \"\".join([ALL_MODELS_README, README_MAPPINGS[prefix]])\n",
    "                    model_to_upload = model_folder / \"wav2vec2-large-xlsr-53-buckeye-ipa\"\n",
    "                readme_path = model_to_upload / \"README.md\"\n",
    "                readme_path.write_text(full_readme)\n",
    "\n",
    "                model_pipeline = transformers.pipeline(\"automatic-speech-recognition\", model=model_to_upload)\n",
    "                print(\"Uploading to hub as:\", hub_name)\n",
    "                model_pipeline.push_to_hub(hub_name)\n",
    "                print(\"Uploading README for\", hub_name)\n",
    "                api.upload_file(\n",
    "                    path_or_fileobj = readme_path, \n",
    "                    path_in_repo = \"README.md\",\n",
    "                    repo_id = hub_name, \n",
    "                    repo_type = \"model\"\n",
    "                )\n",
    "\n",
    "                # Don't look at other prefix keys, the model is already uploaded\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec59f1c180c4e918f1efb90a2067d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/804 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1054280a89314421b866ed1d0cdc178f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/804 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_vcpartridge_umass_edu/multipa/env_cuda124/lib/python3.11/site-packages/librosa/util/files.py:10: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac', 'audio': {'path': '07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00000.flac', 'array': array([ 0.14205933,  0.20620728,  0.27151489, ...,  0.00402832,\n",
      "       -0.00628662, -0.01422119], shape=(238720,)), 'sampling_rate': 16000}, 'duration_ms': 14920, 'text': \"i wanted this to share a few things but i'm going to not share as much as i wanted to share because we are starting late i'd like to get this thing going so we all get home at a decent hour this this election is very important to\"}, {'id': '07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00001.flac', 'audio': {'path': '07282016HFUUforum_SLASH_07-28-2016_HFUUforum_DOT_mp3_00001.flac', 'array': array([-0.01480103,  0.05319214, -0.0105896 , ..., -0.02996826,\n",
      "        0.06680298,  0.0071106 ], shape=(232480,)), 'sampling_rate': 16000}, 'duration_ms': 14530, 'text': \"state we support agriculture to the tune of point four percent no way i made a mistake this year they lowered it from point four percent to point three eight percent and in the same breath they're saying food\"}]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8d53ec53204f59bffdaf52275b74b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e88c00ec2b14d23a0bebf36d7e4463a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1655a7a747c64368b4db154c3dac6110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19827b08bc9d4aa5883ab3a9282462a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/820 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a484ae83da4a6dab91c7e212f9d25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/30.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c954643493c54c7ba7ec61b92982ea8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/548 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec538571e3b340c8bd64da74bdaf7057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual text: i wanted this to share a few things but i'm going to not share as much as i wanted to share because we are starting late i'd like to get this thing going so we all get home at a decent hour this this election is very important to\n",
      "prediction: {'text': 'ɑwɑɾ̃ɪdtɪdʒɪʃʃɛɹfjuθɪŋzbʌɾɑmɡʌɾ̃ʌnɑtʃɛɹʌzmʌtʃɪzɑwɑnɪdɪʃɛɹbikʌzwiɑɹstɑɹɾɪŋleɪʔaɪdlaɪktɪɡɪtðɪsθɪŋɡoʊɪnsʌwiɡɔlɡɪɾhoʊmɛɾɪdisʌnaʊɹ̩ʌmðɪsðɪsʌlɛkʃɪnɪzʌmvɛɹiɪmpɔɹʔn̩tuʌ'}\n",
      "actual text: state we support agriculture to the tune of point four percent no way i made a mistake this year they lowered it from point four percent to point three eight percent and in the same breath they're saying food\n",
      "prediction: {'text': 'steɪʔwisʌpɔɹɾæɡɹɹ̩kʌltʃɹ̩tɪðɪtunʌvpɔnfɔɹpɹ̩sɛnʔoʊnoʊweɪʔaɪmeɪɾʌmʌsteɪkðɪʃjɪɹðeɪloʊɹ̩ɾɪtfɹ̩mpɔntfɔɹpɹ̩sɛnttʌpɔntθɹieɪpɹ̩sɛnʔæɾ̃ɪnnɪseɪmbɹɛθðɹ̩seɪmfuds'}\n"
     ]
    }
   ],
   "source": [
    "# Sanity check that upload worked and the model from the hub can be used for inference\n",
    "from multipa.data_utils import load_buckeye_split\n",
    "import datasets\n",
    "\n",
    "dataset = datasets.load_dataset(\"MLCommons/peoples_speech\", \"clean\", split=\"train\", streaming=True).take(2)\n",
    "dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n",
    "print(list(dataset))\n",
    "pipe = transformers.pipeline(\"automatic-speech-recognition\", model=\"ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_5\")\n",
    "for i in list(dataset): \n",
    "    pred = pipe(i[\"audio\"])\n",
    "    print(\"actual text:\", i[\"text\"])\n",
    "    print(\"prediction:\", pred)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
