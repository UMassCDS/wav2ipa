{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16409b39",
   "metadata": {},
   "source": [
    "# TIMIT Evaluation \n",
    "\n",
    "Runs evaluation scripts on the TIMIT corpus to get phone error rates and edit distances for TIMIT (unseen data) for the following models:\n",
    "- Our models that were fine-tuned on the Buckeye corpus\n",
    "- excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k and excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified models that were trained on TIMIT\n",
    "- C. Taguchi Model\n",
    "- Allosaraus Model\n",
    "- Whisper to Epitran"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e93c94",
   "metadata": {},
   "source": [
    "### Additional installation step for Epitran\n",
    "\n",
    "```bash\n",
    "$ git clone http://github.com/festvox/flite\n",
    "$ cd flite\n",
    "$ ./configure && make\n",
    "$ sudo make install\n",
    "$ cd testsuite\n",
    "$ make lex_lookup\n",
    "$ sudo cp lex_lookup /usr/local/bin\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87390d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "import multipa.evaluation\n",
    "import multipa.evaluation_extras\n",
    "\n",
    "DEVICE = \"mps\"\n",
    "\n",
    "# Paths For TIMIT Database and TIMIT IPA\n",
    "# timit_data_dir = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/COMPLETE\")\n",
    "# transcriptions_path = Path(\"/Users/parthbhangla/Desktop/Multipa_Datasets/TIMIT/complete_ipa.csv\")\n",
    "timit_data_dir = Path(\"../../data/TIMIT Dataset/COMPLETE\")\n",
    "transcriptions_path = Path(\"../../data/TIMIT Dataset/complete_ipa.csv\")\n",
    "\n",
    "# HuggingFace Models Evaluating\n",
    "our_model = \"ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa\"\n",
    "taguchi_1k = \"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\"\n",
    "buckeye_fine_tuned = \"ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3\"\n",
    "\n",
    "# Set up results directories\n",
    "RESULTS_DIR =Path(\"../../data/timit_results\")\n",
    "VERBOSE_RESULTS_DIR = RESULTS_DIR / \"detailed_predictions\"\n",
    "AGGREGATE_METRICS_CSV = RESULTS_DIR / \"aggregate_metrics/all_models_eval.csv\"\n",
    "EDIT_DIST_DIR = RESULTS_DIR / \"edit_distances\"\n",
    "VERBOSE_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "AGGREGATE_METRICS_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "EDIT_DIST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Post-pocessing options\n",
    "IS_REMOVE_SPACES = True\n",
    "IS_NORMALIZE_IPA = True # make common substitutions for IPA compliance using ipatok.tokenise\n",
    "\n",
    "NUM_PROC = 8 # Number of processes for HuggingFace dataset map and filter\n",
    "\n",
    "# Computes and stores by-model performance metrics\n",
    "model_evaluator = multipa.evaluation.ModelEvaluator()\n",
    "\n",
    "evaluated_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8f0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_timit_gold_standard_transcriptions(transcriptions_path):\n",
    "    \"\"\"Returns a dictionary of {\"audio_filename\" -> {\"ipa_transcription\": transcription, \"filename\": original_filename}}\"\"\"\n",
    "    gold_standard_df = pd.read_csv(transcriptions_path)\n",
    "    gold_standard_df[\"filename\"] = gold_standard_df[\"audio_filename\"].str.lower()\n",
    "    gold_standard_df.set_index(\"filename\", inplace=True)\n",
    "    return gold_standard_df.to_dict(\"index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62ccc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total WAV files found: 6300\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR4/MMDM0/SI681.WAV'}, 'filename': '/complete/dr4/mmdm0/si681.wav', 'ipa': ' w ɨ d s ʌ tʃ ɨ n æ k t ɨ v ɹ ɨ f j ʉ ʒ l̩  b i j ʉ s f l̩  '}\n"
     ]
    }
   ],
   "source": [
    "# Load TIMIT audio as a HuggingFace dataset with audio and gold standard transcriptions together\n",
    "# This loads TIMIT as a Dataset with the same columns as the Buckeye corpus we've been working with\n",
    "gold_standard_transcriptions = read_timit_gold_standard_transcriptions(transcriptions_path)\n",
    "\n",
    "timit_wavs = [p for p in timit_data_dir.rglob(\"*\") if p.suffix.lower() == \".wav\"]\n",
    "print(\"Total WAV files found:\", len(timit_wavs))\n",
    "data = []\n",
    "\n",
    "for p in timit_wavs:\n",
    "    clean_filename = \"/\" + str(p.relative_to(timit_data_dir.parent)).lower()\n",
    "    ipa_transcription = gold_standard_transcriptions[clean_filename][\"ipa_transcription\"]\n",
    "\n",
    "    entry = {\n",
    "        \"audio\": {\"path\": str(p)},\n",
    "        \"filename\": clean_filename,\n",
    "        \"ipa\":ipa_transcription\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "audio_dataset = datasets.Dataset.from_list(data)\n",
    "print(audio_dataset)\n",
    "print(audio_dataset[0])\n",
    "\n",
    "# TODO: Evaluate on the whole dataset\n",
    "# Test with a small subset if wanted\n",
    "#audio_subset = audio_dataset.take(10)\n",
    "audio_subset = audio_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14976a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 6300/6300 [00:00<00:00, 38910.95 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 6300/6300 [00:01<00:00, 3936.81 examples/s]\n",
      "Filter (num_proc=8): 100%|██████████| 6300/6300 [00:00<00:00, 11747.44 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio with speech transcriptions\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR4/MMDM0/SI681.WAV', 'array': array([-2.13623047e-04,  6.10351562e-05,  3.05175781e-05, ...,\n",
      "       -3.05175781e-05, -9.15527344e-05, -6.10351562e-05]), 'sampling_rate': 16000}, 'filename': '/complete/dr4/mmdm0/si681.wav', 'ipa': 'wɨdsʌtʃɨnæktɨvɹɨfjʉʒl̩bijʉsfl̩'}\n",
      "Audio without speech transcriptions\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa'],\n",
      "    num_rows: 0\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Sample audio correctly and preprocess transcriptions to remove whitepsace\n",
    "audio_subset, audio_without_speech = multipa.evaluation.preprocess_test_data(audio_subset,\n",
    "    is_remove_space=IS_REMOVE_SPACES, is_normalize_ipa = IS_NORMALIZE_IPA, num_proc=NUM_PROC)\n",
    "print(\"Audio with speech transcriptions\")\n",
    "print(audio_subset)\n",
    "print(audio_subset[0])\n",
    "\n",
    "# Sanity check that there's no audio without transcriptions\n",
    "print(\"Audio without speech transcriptions\")\n",
    "print(audio_without_speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276afb1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating allosaurus. Model: eng2102 Phone inventory: eng\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/allosaurus/am/utils.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(str(path), map_location=torch.device('cpu'))\n",
      "100%|██████████| 6300/6300 [32:07<00:00,  3.27it/s] \n",
      "Map (num_proc=8): 100%|██████████| 6300/6300 [00:00<00:00, 27153.65 examples/s]\n",
      "Flattening the indices: 100%|██████████| 6300/6300 [00:00<00:00, 51212.01 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 138.22ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating Allosaurus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 6300/6300 [00:00<00:00, 52467.21 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng'],\n",
      "    num_rows: 6300\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Allosaurus inference and metrics compute\n",
    "allosaurus_model = \"eng2102\"\n",
    "phone_inventory = \"eng\"\n",
    "allosaurus_model_name = f\"allosaurus_{allosaurus_model}_{phone_inventory}\"\n",
    "\n",
    "# Download model and predict\n",
    "allosaurus_predictions = multipa.evaluation_extras.allosaurus_predict(audio_subset, model=allosaurus_model, phone_inventory=phone_inventory, is_remove_spaces=IS_REMOVE_SPACES, is_normalize_ipa=IS_NORMALIZE_IPA, num_proc=NUM_PROC)\n",
    "\n",
    "# Get evaluation results for raw model output\n",
    "allosaurus_metrics = model_evaluator.eval_non_empty_transcriptions(allosaurus_model_name,\n",
    "    allosaurus_predictions[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"])\n",
    "\n",
    "# Write prediction details and edit distances\n",
    "model_evaluator.write_edit_distance_results(allosaurus_model_name, EDIT_DIST_DIR)\n",
    "multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, allosaurus_model_name, audio_subset, allosaurus_predictions, allosaurus_metrics)\n",
    "\n",
    "# Save model results for later\n",
    "print(\"Done evaluating Allosaurus\")\n",
    "evaluated_models.append(allosaurus_model_name)\n",
    "full_analysis_dataset = audio_subset.add_column(allosaurus_model_name, allosaurus_predictions[multipa.evaluation.PREDICTION_KEY])\n",
    "print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b18c0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ASR for model: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 6300/6300 [00:00<00:00, 18202.09 examples/s]\n",
      "Flattening the indices: 100%|██████████| 6300/6300 [00:00<00:00, 42570.72 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 152.00ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "Running ASR for model: ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 6300/6300 [00:00<00:00, 21959.65 examples/s]\n",
      "Flattening the indices: 100%|██████████| 6300/6300 [00:00<00:00, 45149.28 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 142.13ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3', 'ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "Running ASR for model: ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map (num_proc=8): 100%|██████████| 6300/6300 [00:00<00:00, 22226.62 examples/s]\n",
      "Flattening the indices: 100%|██████████| 6300/6300 [00:00<00:00, 45236.63 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 108.10ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3', 'ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa', 'ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns'],\n",
      "    num_rows: 6300\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace model inference and evaluation\n",
    "# These work with the multipa.evaluation code\n",
    "models = [(buckeye_fine_tuned, IS_NORMALIZE_IPA), (our_model, IS_NORMALIZE_IPA), (taguchi_1k, False)]\n",
    "for model_name, ipa_norm_flag in models:\n",
    "    clean_model_name = multipa.evaluation.clean_model_name(model_name)\n",
    "    print(f\"Running ASR for model: {model_name}\")\n",
    "    asr_pipe = transformers.pipeline(\"automatic-speech-recognition\", model=model_name, device=DEVICE)\n",
    "    predictions_dataset = multipa.evaluation.get_clean_predictions(audio_subset, asr_pipe,\n",
    "        num_proc=NUM_PROC, is_remove_space=IS_REMOVE_SPACES, is_normalize_ipa=ipa_norm_flag)\n",
    "\n",
    "    # Compute all metrics\n",
    "    model_metrics = model_evaluator.eval_non_empty_transcriptions(model_name,\n",
    "        predictions_dataset[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"])\n",
    "\n",
    "    # Write prediction details and edit distances\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, clean_model_name, audio_subset, predictions_dataset, model_metrics)\n",
    "\n",
    "    print(\"Done evaluating\", model_name)\n",
    "    evaluated_models.append(model_name)\n",
    "    full_analysis_dataset = full_analysis_dataset.add_column(name=model_name, column=predictions_dataset[multipa.evaluation.PREDICTION_KEY])\n",
    "    print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2569b509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaulating openai_whisper-large-v3-turbo_to_epitran\n",
      "Building pipeline and downloading model\n",
      "Predicting with openai/whisper-large-v3-turbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:483: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "You have passed language=english, but also have set `forced_decoder_ids` to [[1, None], [2, 50360]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of language=english.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transliterating with Epitran\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [11:48<00:00,  8.89it/s]\n",
      "Parameter 'function'=<function hf_model_to_epitran_predict.<locals>.<lambda> at 0x804ff2980> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function hf_model_to_epitran_predict.<locals>.<lambda> at 0x804ff2980> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map (num_proc=8): 100%|██████████| 6300/6300 [00:00<00:00, 29856.98 examples/s]\n",
      "Flattening the indices: 100%|██████████| 6300/6300 [00:00<00:00, 37378.95 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 116.76ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating openai_whisper-large-v3-turbo_to_epitran\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3', 'ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa', 'ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns', 'openai_whisper-large-v3-turbo_to_epitran'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "Evaulating openai_whisper-medium.en_to_epitran\n",
      "Building pipeline and downloading model\n",
      "Predicting with openai/whisper-medium.en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/transformers/models/whisper/generation_whisper.py:483: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transliterating with Epitran\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6300/6300 [12:14<00:00,  8.57it/s]\n",
      "Map (num_proc=8): 100%|██████████| 6300/6300 [00:00<00:00, 26392.52 examples/s]\n",
      "Flattening the indices: 100%|██████████| 6300/6300 [00:00<00:00, 23456.87 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 115.99ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done evaluating openai_whisper-medium.en_to_epitran\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3', 'ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa', 'ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns', 'openai_whisper-large-v3-turbo_to_epitran', 'openai_whisper-medium.en_to_epitran'],\n",
      "    num_rows: 6300\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Orthographic to epitran models\n",
    "models = [\n",
    "    \"openai/whisper-large-v3-turbo\",\n",
    "    # \"openai/whisper-large-v3\",\n",
    "    \"openai/whisper-medium.en\",\n",
    "]\n",
    "for m in models:\n",
    "    model_name = f\"{m}_to_epitran\".replace(\"/\", \"_\")\n",
    "    print(\"Evaulating\", model_name)\n",
    "\n",
    "    # Download model and predict\n",
    "    epitran_predictions = multipa.evaluation_extras.hf_model_to_epitran_predict(m, audio_subset, device=DEVICE, num_proc=NUM_PROC, is_remove_spaces=IS_REMOVE_SPACES, is_normalize_ipa=IS_NORMALIZE_IPA)\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name, epitran_predictions[multipa.evaluation.PREDICTION_KEY], audio_subset[\"ipa\"]\n",
    "    )\n",
    "    multipa.evaluation.write_detailed_prediction_results(VERBOSE_RESULTS_DIR, model_name, audio_subset, epitran_predictions, metrics)\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    print(\"Done evaluating\", model_name)\n",
    "    evaluated_models.append(model_name)\n",
    "    full_analysis_dataset = full_analysis_dataset.add_column(name=model_name, column=epitran_predictions[multipa.evaluation.PREDICTION_KEY])\n",
    "    print(full_analysis_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd9b7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Map: 100%|██████████| 6300/6300 [00:00<00:00, 8465.08 examples/s]\n",
      "Map: 100%|██████████| 6300/6300 [00:00<00:00, 67595.21 examples/s]\n",
      "Flattening the indices: 100%|██████████| 6300/6300 [00:00<00:00, 49523.24 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 164.57ba/s]\n",
      "Some weights of the model checkpoint at excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Map: 100%|██████████| 6300/6300 [00:00<00:00, 8735.68 examples/s]\n",
      "Map: 100%|██████████| 6300/6300 [00:00<00:00, 70162.14 examples/s]\n",
      "Flattening the indices: 100%|██████████| 6300/6300 [00:00<00:00, 50314.68 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 156.20ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Models fine-tuned on TIMIT\n",
    "hf_to_phonecodes_models = [(\"excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k\", \"timit\", \"ipa\"), (\"excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified\", \"timit\", \"ipa\")]\n",
    "\n",
    "for model_name, in_code, out_code in hf_to_phonecodes_models:\n",
    "    model_predictions = multipa.evaluation_extras.hf_to_phonecodes(audio_subset, model_name, in_code, out_code)\n",
    "\n",
    "    metrics = model_evaluator.eval_non_empty_transcriptions(\n",
    "        model_name,\n",
    "        model_predictions[multipa.evaluation.PREDICTION_KEY],\n",
    "        audio_subset[\"ipa\"])\n",
    "    multipa.evaluation.write_detailed_prediction_results(\n",
    "        VERBOSE_RESULTS_DIR, multipa.evaluation.clean_model_name(model_name), audio_subset, model_predictions, metrics\n",
    "    )\n",
    "    model_evaluator.write_edit_distance_results(model_name, EDIT_DIST_DIR)\n",
    "    evaluated_models.append(model_name)\n",
    "    full_analysis_dataset = full_analysis_dataset.add_column(name=model_name, column=model_predictions[multipa.evaluation.PREDICTION_KEY])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4281ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all results to file for comparison\n",
    "model_evaluator.to_csv(AGGREGATE_METRICS_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e6e7994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These models were evaluated: ['allosaurus_eng2102_eng', 'ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3', 'ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa', 'ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns', 'openai_whisper-large-v3-turbo_to_epitran', 'openai_whisper-medium.en_to_epitran', 'excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k', 'excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified']\n",
      "Dataset snippet for full anslysis:\n",
      "Dataset({\n",
      "    features: ['audio', 'filename', 'ipa', 'allosaurus_eng2102_eng', 'ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3', 'ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa', 'ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns', 'openai_whisper-large-v3-turbo_to_epitran', 'openai_whisper-medium.en_to_epitran', 'excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k', 'excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified'],\n",
      "    num_rows: 6300\n",
      "})\n",
      "{'audio': {'path': '../../data/TIMIT Dataset/COMPLETE/DR4/MMDM0/SI681.WAV', 'array': array([-2.13623047e-04,  6.10351562e-05,  3.05175781e-05, ...,\n",
      "       -3.05175781e-05, -9.15527344e-05, -6.10351562e-05]), 'sampling_rate': 16000}, 'filename': '/complete/dr4/mmdm0/si681.wav', 'ipa': 'wɨdsʌtʃɨnæktɨvɹɨfjʉʒl̩bijʉsfl̩', 'allosaurus_eng2102_eng': 'wɪðsʌt͡ʃænæktʌvɹəfjuzəlbijusfəl', 'ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3': 'wʊdsʌtʃɪnæktɪvɹɪfjuzʊlbijusfl̩', 'ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa': 'wɪdsʌtʃɪnæktɪvɹɪfjuʒl̩bijusfl̩', 'ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns': 'wɨtsat͡ɕinaktɨrɨfizo̞bɨjusfo', 'openai_whisper-large-v3-turbo_to_epitran': 'wʊdsʌt͡ʃænæktʌvɹəfjuzəlbijusfəlʔ', 'openai_whisper-medium.en_to_epitran': 'wʊdsʌt͡ʃænæktʌvɹəfjuzəlbijusfəlʔ', 'excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k': 'wʊdsʌtʃɨnæktɨvɹɨfjʉzl̩bijʉsfl̩', 'excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified': 'wʊdsʌtʃɪnæktɪvɹɪfjuʃl̩bijusfl̩'}\n"
     ]
    }
   ],
   "source": [
    "print(\"These models were evaluated:\", evaluated_models)\n",
    "print(\"Dataset snippet for full anslysis:\")\n",
    "print(full_analysis_dataset)\n",
    "print(full_analysis_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2219485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions_df snippet\n",
      "                                               audio  \\\n",
      "0  {'bytes': None, 'path': '../../data/TIMIT Data...   \n",
      "1  {'bytes': None, 'path': '../../data/TIMIT Data...   \n",
      "2  {'bytes': None, 'path': '../../data/TIMIT Data...   \n",
      "3  {'bytes': None, 'path': '../../data/TIMIT Data...   \n",
      "4  {'bytes': None, 'path': '../../data/TIMIT Data...   \n",
      "\n",
      "                        filename                                     ipa  \\\n",
      "0  /complete/dr4/mmdm0/si681.wav          wɨdsʌtʃɨnæktɨvɹɨfjʉʒl̩bijʉsfl̩   \n",
      "1    /complete/dr4/mmdm0/sa2.wav          doʊɾ̃æsmiɾɨkɪɹiɛɾ̃ɔliɹæɡlʌkðæt   \n",
      "2  /complete/dr4/mmdm0/sx411.wav  bʌɾə˞skɑtʃfʌdʒɡoʊzwɛlwəðvəɾ̃ɪləaɪskɹim   \n",
      "3    /complete/dr4/mmdm0/sa1.wav      ʃiædjə˞dɑɹksʉɾɨnɡɹiziwɔʃwɑɾə˞ɔljɪɹ   \n",
      "4  /complete/dr4/mmdm0/sx231.wav                           ʔɑʔɑɾ̃ə˞mɑmɑm   \n",
      "\n",
      "                   allosaurus_eng2102_eng  \\\n",
      "0         wɪðsʌt͡ʃænæktʌvɹəfjuzəlbijusfəl   \n",
      "1          dɑnæsktmitəkæɹiɪnowliɹɛɡlɛkðæt   \n",
      "2  bɛtɹ̩skɑt͡ʃfɹ̩d͡ʒɡowzwɛlwɑzvənilæskɹjn   \n",
      "3      ʃiædjɔdɑɹksutɪnɡɹeiziwɑʃwɔtɹ̩ɔljɪɹ   \n",
      "4                            ʌajtɹ̩majmʌm   \n",
      "\n",
      "  ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3  \\\n",
      "0                     wʊdsʌtʃɪnæktɪvɹɪfjuzʊlbijusfl̩                           \n",
      "1                  doʊɾ̃æskmiɾɪkɛɹiɛɾ̃oʊliɹæɡlaɪkðæʔ                           \n",
      "2               bʌɾɹ̩skɑtʃfʌdʒɡoʊzwɛlwʊðvʌnɛlʌæskɹiŋ                           \n",
      "3                 ʃiædjɹ̩dɑɹksuɾɪŋɡɹiziwɑʃwɔɾɹ̩ɔljiɹ                           \n",
      "4                                        ɑɑɾ̃ɹ̩mɑmɑm                           \n",
      "\n",
      "  ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa  \\\n",
      "0                     wɪdsʌtʃɪnæktɪvɹɪfjuʒl̩bijusfl̩              \n",
      "1                   doʊnæskmiɾɪkɛɹiɛɾ̃oʊliɹæɡlɛktðæʔ              \n",
      "2                bɛɾɹ̩skɑtʃfʌdʒɡoʊzwɛʌwʊzvʌnɛlæskɹim              \n",
      "3                ʃiɛdjɹ̩dɑɹksuɾɪŋɡɹiziwʌʃwɔɾɹ̩ɛlʌjiɹ              \n",
      "4                                         ʌɔɾɹ̩mɑmɑm              \n",
      "\n",
      "  ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns  \\\n",
      "0                       wɨtsat͡ɕinaktɨrɨfizo̞bɨjusfo       \n",
      "1                     nooneːsmirukeɻienɒlujɻæːɡlɛkte       \n",
      "2           bɛrørskaːt͡ʃfɒjd͡ʑkɛuzvɛotvneːlaaːskrejɲ       \n",
      "3                  ɕiærjydɑːkjsɯyringɻiziwɑʃwɑrɑojiː       \n",
      "4                                          ɛanmamaːm       \n",
      "\n",
      "  openai_whisper-large-v3-turbo_to_epitran  \\\n",
      "0         wʊdsʌt͡ʃænæktʌvɹəfjuzəlbijusfəlʔ   \n",
      "1           dɑntæskmitəkæɹiænɑliɹæɡlajkðæt   \n",
      "2  bʌtɹ̩skɑt͡ʃfʌd͡ʒɡowzwɛlwɪðvənɪləajskɹim   \n",
      "3     ʃihædjɔɹdɑɹksutændɡɹisiwɑʃwɔtɹ̩ɔljɪɹ   \n",
      "4                             ajɑnɹ̩majmɑm   \n",
      "\n",
      "       openai_whisper-medium.en_to_epitran  \\\n",
      "0         wʊdsʌt͡ʃænæktʌvɹəfjuzəlbijusfəlʔ   \n",
      "1         dɑntæskmitəkæɹiænowlajɹæɡlajkðæt   \n",
      "2  bʌtɹ̩skɑt͡ʃfʌd͡ʒɡowzwɛlwɪðvənɪləajskɹim   \n",
      "3     ʃihædjɔɹdɑɹksutændɡɹisiwɑʃwɔtɹ̩ɔljɪɹ   \n",
      "4                             ajɑnɹ̩majmɑm   \n",
      "\n",
      "  excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k  \\\n",
      "0                     wʊdsʌtʃɨnæktɨvɹɨfjʉzl̩bijʉsfl̩               \n",
      "1                  doʊɾ̃æskmiɾɨkɛɹiɨɾ̃oʊliɹæɡlaɪkðæt               \n",
      "2               bʌɾɚskɑtʃfʌdʒɡoʊzwɛlwəðvɨnɪləaɪskɹim               \n",
      "3                   ʃiædjɚdɑɹksʉɾɨŋɡɹiziwɔʃwɔɾɚɔljɪɚ               \n",
      "4                                       ʔɑʔɑɾ̃ɚmɑmɑm               \n",
      "\n",
      "  excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified  \n",
      "0                     wʊdsʌtʃɪnæktɪvɹɪfjuʃl̩bijusfl̩                         \n",
      "1                    doʊnæskmiɾɪkɛɹiɪnoʊliɹæɡlaɪkðæt                         \n",
      "2               bʌɾɝskɑtʃfʌdʒɡoʊzwɛlwəðvɪnɪləaɪskɹim                         \n",
      "3                   ʃiædjɝdɑɹksuɾɪŋɡɹiziwɔʃwɔɾɝɔljɪɝ                         \n",
      "4                                        ʔɑʔɑnɝmɑmɑm                         \n",
      "full_comparison_df snippet\n",
      "                        filename                                     ipa  \\\n",
      "0  /complete/dr4/mmdm0/si681.wav          wɨdsʌtʃɨnæktɨvɹɨfjʉʒl̩bijʉsfl̩   \n",
      "1    /complete/dr4/mmdm0/sa2.wav          doʊɾ̃æsmiɾɨkɪɹiɛɾ̃ɔliɹæɡlʌkðæt   \n",
      "2  /complete/dr4/mmdm0/sx411.wav  bʌɾə˞skɑtʃfʌdʒɡoʊzwɛlwəðvəɾ̃ɪləaɪskɹim   \n",
      "3    /complete/dr4/mmdm0/sa1.wav      ʃiædjə˞dɑɹksʉɾɨnɡɹiziwɔʃwɑɾə˞ɔljɪɹ   \n",
      "4  /complete/dr4/mmdm0/sx231.wav                           ʔɑʔɑɾ̃ə˞mɑmɑm   \n",
      "\n",
      "                   allosaurus_eng2102_eng  \\\n",
      "0         wɪðsʌt͡ʃænæktʌvɹəfjuzəlbijusfəl   \n",
      "1          dɑnæsktmitəkæɹiɪnowliɹɛɡlɛkðæt   \n",
      "2  bɛtɹ̩skɑt͡ʃfɹ̩d͡ʒɡowzwɛlwɑzvənilæskɹjn   \n",
      "3      ʃiædjɔdɑɹksutɪnɡɹeiziwɑʃwɔtɹ̩ɔljɪɹ   \n",
      "4                            ʌajtɹ̩majmʌm   \n",
      "\n",
      "  ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3  \\\n",
      "0                     wʊdsʌtʃɪnæktɪvɹɪfjuzʊlbijusfl̩                           \n",
      "1                  doʊɾ̃æskmiɾɪkɛɹiɛɾ̃oʊliɹæɡlaɪkðæʔ                           \n",
      "2               bʌɾɹ̩skɑtʃfʌdʒɡoʊzwɛlwʊðvʌnɛlʌæskɹiŋ                           \n",
      "3                 ʃiædjɹ̩dɑɹksuɾɪŋɡɹiziwɑʃwɔɾɹ̩ɔljiɹ                           \n",
      "4                                        ɑɑɾ̃ɹ̩mɑmɑm                           \n",
      "\n",
      "  ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa  \\\n",
      "0                     wɪdsʌtʃɪnæktɪvɹɪfjuʒl̩bijusfl̩              \n",
      "1                   doʊnæskmiɾɪkɛɹiɛɾ̃oʊliɹæɡlɛktðæʔ              \n",
      "2                bɛɾɹ̩skɑtʃfʌdʒɡoʊzwɛʌwʊzvʌnɛlæskɹim              \n",
      "3                ʃiɛdjɹ̩dɑɹksuɾɪŋɡɹiziwʌʃwɔɾɹ̩ɛlʌjiɹ              \n",
      "4                                         ʌɔɾɹ̩mɑmɑm              \n",
      "\n",
      "  ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns  \\\n",
      "0                       wɨtsat͡ɕinaktɨrɨfizo̞bɨjusfo       \n",
      "1                     nooneːsmirukeɻienɒlujɻæːɡlɛkte       \n",
      "2           bɛrørskaːt͡ʃfɒjd͡ʑkɛuzvɛotvneːlaaːskrejɲ       \n",
      "3                  ɕiærjydɑːkjsɯyringɻiziwɑʃwɑrɑojiː       \n",
      "4                                          ɛanmamaːm       \n",
      "\n",
      "  openai_whisper-large-v3-turbo_to_epitran  \\\n",
      "0         wʊdsʌt͡ʃænæktʌvɹəfjuzəlbijusfəlʔ   \n",
      "1           dɑntæskmitəkæɹiænɑliɹæɡlajkðæt   \n",
      "2  bʌtɹ̩skɑt͡ʃfʌd͡ʒɡowzwɛlwɪðvənɪləajskɹim   \n",
      "3     ʃihædjɔɹdɑɹksutændɡɹisiwɑʃwɔtɹ̩ɔljɪɹ   \n",
      "4                             ajɑnɹ̩majmɑm   \n",
      "\n",
      "       openai_whisper-medium.en_to_epitran  \\\n",
      "0         wʊdsʌt͡ʃænæktʌvɹəfjuzəlbijusfəlʔ   \n",
      "1         dɑntæskmitəkæɹiænowlajɹæɡlajkðæt   \n",
      "2  bʌtɹ̩skɑt͡ʃfʌd͡ʒɡowzwɛlwɪðvənɪləajskɹim   \n",
      "3     ʃihædjɔɹdɑɹksutændɡɹisiwɑʃwɔtɹ̩ɔljɪɹ   \n",
      "4                             ajɑnɹ̩majmɑm   \n",
      "\n",
      "  excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k  \\\n",
      "0                     wʊdsʌtʃɨnæktɨvɹɨfjʉzl̩bijʉsfl̩               \n",
      "1                  doʊɾ̃æskmiɾɨkɛɹiɨɾ̃oʊliɹæɡlaɪkðæt               \n",
      "2               bʌɾɚskɑtʃfʌdʒɡoʊzwɛlwəðvɨnɪləaɪskɹim               \n",
      "3                   ʃiædjɚdɑɹksʉɾɨŋɡɹiziwɔʃwɔɾɚɔljɪɚ               \n",
      "4                                       ʔɑʔɑɾ̃ɚmɑmɑm               \n",
      "\n",
      "  excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified  \n",
      "0                     wʊdsʌtʃɪnæktɪvɹɪfjuʃl̩bijusfl̩                         \n",
      "1                    doʊnæskmiɾɪkɛɹiɪnoʊliɹæɡlaɪkðæt                         \n",
      "2               bʌɾɝskɑtʃfʌdʒɡoʊzwɛlwəðvɪnɪləaɪskɹim                         \n",
      "3                   ʃiædjɝdɑɹksuɾɪŋɡɹiziwɔʃwɔɾɝɔljɪɝ                         \n",
      "4                                        ʔɑʔɑnɝmɑmɑm                         \n"
     ]
    }
   ],
   "source": [
    "predictions_df = full_analysis_dataset.to_pandas()\n",
    "print(\"predictions_df snippet\")\n",
    "print(predictions_df.head())\n",
    "\n",
    "full_comparison_df = predictions_df.drop(\n",
    "    columns=[\"audio\"]\n",
    "    )\n",
    "\n",
    "print(\"full_comparison_df snippet\")\n",
    "print(full_comparison_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4f87c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['filename', 'ipa', 'allosaurus_eng2102_eng',\n",
       "       'ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3',\n",
       "       'ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa',\n",
       "       'ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns',\n",
       "       'openai_whisper-large-v3-turbo_to_epitran',\n",
       "       'openai_whisper-medium.en_to_epitran',\n",
       "       'excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k',\n",
       "       'excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_comparison_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3494be44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialect groups found: ['DR4' 'DR3' 'DR2' 'DR5' 'DR7' 'DR6' 'DR1' 'DR8']\n",
      "Evaluating model: allosaurus_eng2102_eng\n",
      "Evaluating model: ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3\n",
      "Evaluating model: ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa\n",
      "Evaluating model: ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\n",
      "Evaluating model: openai_whisper-large-v3-turbo_to_epitran\n",
      "Evaluating model: openai_whisper-medium.en_to_epitran\n",
      "Evaluating model: excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k\n",
      "Evaluating model: excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_simplified\n",
      "Average evaluation metrics per model saved to timit_model_evaluation_summary.csv\n",
      "Dialect evaluation complete. Results saved to timit_dialect_model_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "# Finalize aggregate and by-dialect results\n",
    "gold_col = \"ipa\"\n",
    "model_names = evaluated_models\n",
    "model_eval = multipa.evaluation.ModelEvaluator()\n",
    "\n",
    "def extract_dialect(path_str):\n",
    "    path = Path(path_str)\n",
    "    parts = [p for p in path.parts if p.lower().startswith(\"dr\")]\n",
    "    return parts[0].upper() if parts else \"UNKNOWN\"\n",
    "\n",
    "full_comparison_df[\"dialect\"] = full_comparison_df[\"filename\"].apply(extract_dialect)\n",
    "print(\"Dialect groups found:\", full_comparison_df[\"dialect\"].unique())\n",
    "\n",
    "summary_data = {}\n",
    "dialect_results = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "\n",
    "    predictions = full_comparison_df[model_name].tolist()\n",
    "    references = full_comparison_df[gold_col].tolist()\n",
    "\n",
    "    metrics = model_eval.eval_non_empty_transcriptions(model_name, predictions, references)\n",
    "\n",
    "    for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "        col_name = f\"{metric_name} VS {model_name}\"\n",
    "        full_comparison_df[col_name] = metrics[metric_name]\n",
    "\n",
    "    summary_data[model_name] = {\n",
    "        metric_name: float(np.mean(metrics[metric_name]))\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]\n",
    "    }\n",
    "\n",
    "    for dialect, df_group in full_comparison_df.groupby(\"dialect\"):\n",
    "        result_row = {\n",
    "            \"dialect\": dialect,\n",
    "            \"model\": model_name,\n",
    "        }\n",
    "        for metric_name in [\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]:\n",
    "            col_name = f\"{metric_name} VS {model_name}\"\n",
    "            result_row[metric_name] = df_group[col_name].mean()\n",
    "        dialect_results.append(result_row)\n",
    "\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data).T\n",
    "summary_df = summary_df[[\"phone_error_rates\", \"phone_feature_error_rates\", \"feature_error_rates\"]]\n",
    "summary_df = summary_df.reset_index()\n",
    "summary_df = summary_df.rename(columns={\"index\": \"model\"})\n",
    "summary_df.to_csv(\"timit_model_evaluation_summary.csv\", index=False)\n",
    "print(\"Average evaluation metrics per model saved to timit_model_evaluation_summary.csv\")\n",
    "\n",
    "\n",
    "dialect_summary_df = pd.DataFrame(dialect_results)\n",
    "dialect_summary_df.to_csv(\"timit_dialect_model_comparison.csv\", index=False)\n",
    "print(\"Dialect evaluation complete. Results saved to timit_dialect_model_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
