{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d77c8865",
   "metadata": {},
   "source": [
    "# TIMIT Test split performance analysis\n",
    "This analyzes model performance on the test split TIMIT corpus, with special attention to performance on vowels. Reduction to the shared Buckeye/TIMIT symbol set is performed before analysis and performance metrics are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cab68502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/virginia/miniconda3/envs/multipa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "import datasets\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from phonecodes import phonecodes, phonecode_tables\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import multipa.evaluation\n",
    "import multipa.evaluation_extras\n",
    "\n",
    "# Visualiation settings\n",
    "PALETTE = \"gist_gray\"\n",
    "CONTEXT = \"paper\"\n",
    "FONT_SCALE = 2\n",
    "\n",
    "sns.color_palette(PALETTE)\n",
    "sns.set_context(context=CONTEXT, font_scale=FONT_SCALE)\n",
    "# font = {\"size\": 16}\n",
    "# matplotlib.rc(\"font\", **font)\n",
    "# Remove the limits on the number of rows displayed in the notebook\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6de47879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data processing settings\n",
    "TIMIT_DATASET = Path(\"../../data/TIMIT\")\n",
    "IPA_KEY = \"ipa\"\n",
    "PREDICTION_KEY = \"prediction\"\n",
    "NUM_PROC = 8\n",
    "\n",
    "# Evaluation results\n",
    "TIMIT_EVAL_DIR = Path(\"../../data/timit_results/\")\n",
    "\n",
    "DETAILED_PRED_DIR = TIMIT_EVAL_DIR / \"detailed_predictions\"\n",
    "EDIT_DIST_DIR = TIMIT_EVAL_DIR / \"edit_distances\"\n",
    "\n",
    "# Full original set, some may be removed after symbol reduction\n",
    "TIMIT_VOWELS = [\"ɑ\", \"æ\", \"ʌ\", \"ɔ\", \"ɛ\", \"ɪ\", \"i\", \"ʊ\", \"u\", \"ə\", \"ə̥\", \"ʉ\", \"ɨ\", \"ɹ̩\", \"ɚ\"]\n",
    "TIMIT_DIPHTHONGS = [\"aʊ\", \"eɪ\", \"aɪ\",  \"oʊ\", \"ɔɪ\"]\n",
    "\n",
    "DIALECT_REGIONS = {\n",
    "    \"DR1\": \"DR1: New England\",\n",
    "    \"DR2\": \"DR2: Northern\",\n",
    "    \"DR3\": \"DR3: North Midland\",\n",
    "    \"DR4\": \"DR4: South Midland\",\n",
    "    \"DR5\": \"DR5: Southern\",\n",
    "    \"DR6\": \"DR6: New York City\",\n",
    "    \"DR7\": \"DR7: Western\",\n",
    "    \"DR8\": \"DR8: Army Brat\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a15357d",
   "metadata": {},
   "source": [
    "## Basic model performance comparisons\n",
    "Show performance metrics for each model on TIMIT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33aa1b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually define and join model source description\n",
    "\n",
    "# Models fine-tuned on Buckeye, post process using BUCKEYE_REDUCED_MAPPING\n",
    "wav2ipa_models = [\n",
    "    (\"ginic/full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa\", \"Buckeye fine-tuned on full train split\"),\n",
    "    (\"ginic/wav2vec2-large-lv60_phoneme-timit_english_timit-4k_buckeye-4k_bs32_3\", \"Lee 2025 fine-tuned again on Buckeye\"),\n",
    "    ]\n",
    "\n",
    "# External models, post process using\n",
    "external_models = [\n",
    "    (\"excalibur12/wav2vec2-large-lv60_phoneme-timit_english_timit-4k\", \"Lee 2025 Wav2Vec2.0 TIMIT fine-tuned\"),\n",
    "\n",
    "    (\"openai_whisper-medium.en_to_epitran\", \"Whisper + Epitran\"),\n",
    "    (\"facebook/wav2vec2-lv-60-espeak-cv-ft\", \"facebook/wav2vec2-lv-60-espeak-cv-ft\"),\n",
    "    (\"facebook/wav2vec2-xlsr-53-espeak-cv-ft\", \"facebook/wav2vec2-xlsr-53-espeak-cv-ft\"),\n",
    "    (\"allosaurus_eng2102_eng\", \"Allosaurus English\"),\n",
    "    (\"ctaguchi/wav2vec2-large-xlsr-japlmthufielta-ipa1000-ns\", \"Taguchi et al. 2023\"),\n",
    "    # Intentionally omitting Whisper large - let's just keep the best in each category\n",
    "    # (\"openai_whisper-large-v3-turbo_to_epitran\", \"Whisper + Epitran\"),\n",
    "]\n",
    "\n",
    "hue_order = [t[1] for t in wav2ipa_models + external_models]\n",
    "\n",
    "model_sources_df = pd.DataFrame(wav2ipa_models + external_models, columns=[\"model\", \"Model source\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a559e14a",
   "metadata": {},
   "source": [
    "# Read and preprocess TIMIT test set\n",
    "Performs symbol reduction on the TIMIT test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06c69d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMIT Test Size: 1680\n",
      "TIMIT Test Snippet: {'file': '/Users/virginia/workspace/multipa/data/TIMIT/TEST/DR1/FAKS0/SA1.WAV', 'audio': {'path': '/Users/virginia/workspace/multipa/data/TIMIT/TEST/DR1/FAKS0/SA1.WAV', 'array': array([9.15527344e-05, 1.52587891e-04, 6.10351562e-05, ...,\n",
      "       2.44140625e-04, 3.05175781e-04, 2.13623047e-04]), 'sampling_rate': 16000}, 'text': 'She had your dark suit in greasy wash water all year.', 'phonetic_detail': {'start': [0, 9640, 11240, 12783, 14078, 16157, 16880, 17103, 17587, 18760, 19720, 19962, 21514, 22680, 23800, 24104, 26280, 28591, 29179, 30337, 31880, 32500, 33170, 33829, 35150, 37370, 38568, 40546, 42357, 45119, 45624, 46855, 48680, 49240, 51033, 52378, 54500, 55461, 57395, 59179, 60600], 'stop': [9640, 11240, 12783, 14078, 16157, 16880, 17103, 17587, 18760, 19720, 19962, 21514, 22680, 23800, 24104, 26280, 28591, 29179, 30337, 31880, 32500, 33170, 33829, 35150, 37370, 38568, 40546, 42357, 45119, 45624, 46855, 48680, 49240, 51033, 52378, 54500, 55461, 57395, 59179, 60600, 63440], 'utterance': ['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'd', 'y', 'er', 'dcl', 'd', 'aa', 'r', 'kcl', 'k', 's', 'uw', 'dx', 'ih', 'ng', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'er', 'q', 'ao', 'l', 'y', 'iy', 'axr', 'h#']}, 'word_detail': {'start': [9640, 12783, 17103, 18760, 24104, 29179, 31880, 38568, 45624, 52378, 55461], 'stop': [12783, 17103, 18760, 24104, 29179, 31880, 38568, 45119, 51033, 55461, 60600], 'utterance': ['she', 'had', 'your', 'dark', 'suit', 'in', 'greasy', 'wash', 'water', 'all', 'year']}, 'dialect_region': 'DR1', 'sentence_type': 'SA', 'speaker_id': 'AKS0', 'id': 'SA1'}\n",
      "['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'd', 'y', 'er', 'dcl', 'd', 'aa', 'r', 'kcl', 'k', 's', 'uw', 'dx', 'ih', 'ng', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'er', 'q', 'ao', 'l', 'y', 'iy', 'axr', 'h#']\n"
     ]
    }
   ],
   "source": [
    "timit_test_dataset = datasets.load_dataset(\"timit_asr\", data_dir = TIMIT_DATASET, split=\"test\")\n",
    "print(\"TIMIT Test Size:\", len(timit_test_dataset))\n",
    "print(\"TIMIT Test Snippet:\", timit_test_dataset[0])\n",
    "print(timit_test_dataset[0][\"phonetic_detail\"][\"utterance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c430eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_timit_to_ipa(dataset_entry:dict[str, Any]):\n",
    "    timit_str = dataset_entry[\"phonetic_detail\"][\"utterance\"]\n",
    "    ipa_syms = phonecodes.timit2ipa(\" \".join(timit_str)).split()\n",
    "    dataset_entry[IPA_KEY] = \"\".join(ipa_syms)\n",
    "    return dataset_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23162a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'd', 'y', 'er', 'dcl', 'd', 'aa', 'r', 'kcl', 'k', 's', 'uw', 'dx', 'ih', 'ng', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'er', 'q', 'ao', 'l', 'y', 'iy', 'axr', 'h#']\n",
      "ʃiɦædjɝdɑɹksuɾɪŋɡɹisiwɑʃwɑɾɝʔɔljiɚ\n"
     ]
    }
   ],
   "source": [
    "# Convert original TIMIT ARPABET to IPA\n",
    "timit_test_with_ipa = timit_test_dataset.map(batch_timit_to_ipa, num_proc=NUM_PROC)\n",
    "print(timit_test_with_ipa[0][\"phonetic_detail\"][\"utterance\"])\n",
    "print(timit_test_with_ipa[0][IPA_KEY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0969896a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|██████████| 1680/1680 [00:00<00:00, 9748.28 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ʃihædjɹ̩dɑɹksuɾɪŋɡɹisiwɑʃwɑɾɹ̩ʔɔljiɹ̩\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Post process IPA to reduced TIMIT symbol set\n",
    "timit_test_with_ipa = multipa.evaluation_extras.dataset_reduction_greedy_find_and_replace(timit_test_with_ipa, IPA_KEY, \"timit\", num_proc=NUM_PROC)\n",
    "print(timit_test_with_ipa[0][IPA_KEY])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d6997",
   "metadata": {},
   "source": [
    "# Dialect Region Performance Plots\n",
    "This creates bar charts showing performance by dialect. Since the groupby and averaging was already done, we just need to read in the data and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d0bc86d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_sources' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read in predictions and extract dialect region\u001b[39;00m\n\u001b[1;32m      2\u001b[0m detailed_results_dfs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model, label \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel_sources\u001b[49m:\n\u001b[1;32m      4\u001b[0m     clean_model_name \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     tmp_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DETAILED_PRED_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclean_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_detailed_predictions.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_sources' is not defined"
     ]
    }
   ],
   "source": [
    "# Read in predictions and extract dialect region\n",
    "detailed_results_dfs = []\n",
    "for model, label in model_sources:\n",
    "    clean_model_name = model.replace(\"/\", \"_\")\n",
    "    tmp_df = pd.read_csv(DETAILED_PRED_DIR / f\"{clean_model_name}_detailed_predictions.csv\")\n",
    "    tmp_df[\"model_name\"] = model\n",
    "    tmp_df[\"Model source\"] = label\n",
    "    detailed_results_dfs.append(tmp_df)\n",
    "\n",
    "detailed_preds_df = pd.concat(detailed_results_dfs)\n",
    "detailed_preds_df[\"dialect\"] = detailed_preds_df[\"filename\"].apply(lambda x: x.split(\"/\")[2].upper())\n",
    "display(detailed_preds_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5360a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show performance by dialect region\n",
    "dialect_df = detailed_preds_df.groupby([\"model_name\", \"Model source\", \"dialect\"])[\"phone_error_rates\"].mean().reset_index()\n",
    "display(dialect_df.head())\n",
    "dialect_df = dialect_df.merge(pd.DataFrame(DIALECT_REGIONS.items(), columns=[\"dialect\", \"Dialect Region\"]), on=\"dialect\")\n",
    "dialect_df =  dialect_df.sort_values(by=[\"Dialect Region\", \"phone_error_rates\"], ascending=[True, True])\n",
    "display(dialect_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae7d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(dialect_df, col=\"Dialect Region\", col_wrap=4, height=4, aspect=0.75)\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.map_dataframe(sns.barplot, y=\"phone_error_rates\", hue=\"Model source\", palette=PALETTE, hue_order = hue_order)\n",
    "g.add_legend(title=\"Model source\")\n",
    "g.set_ylabels(\"Average Phone Error Rate\")\n",
    "g.fig.suptitle(\"Models' Average Phone Error Rates by Dialect Region\", fontsize=24, y=1.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ccda0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialect performance for just our model\n",
    "our_model_dialect_df = dialect_df[dialect_df[\"Model source\"] == \"Our AutoIPA: fine-tuned on full train split\"]\n",
    "g = sns.barplot(data=our_model_dialect_df, y=\"Dialect Region\", x=\"phone_error_rates\", hue=\"Dialect Region\", palette=PALETTE)\n",
    "g.set_xlabel(\"Average Phone Error Rate\")\n",
    "g.set_xlim((0,0.5))\n",
    "g.set(title=\"Our AutoIPA's TIMIT Performance by Dialect Region\")\n",
    "for bar in g.containers:\n",
    "    g.bar_label(bar, fmt=\"%.2f\", padding=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172b303a",
   "metadata": {},
   "source": [
    "# Vowel Error Rate Analysis\n",
    "How many instances of each vowel in the vocabulary are we getting wrong? \n",
    "$$ error\\_rate(v) = \\frac{count\\_substitutions\\_of(v) + count\\_deletions(v)}{total\\_count(v)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0153e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_transcription_df = pd.read_csv(GOLD_TRANSCRIPTIONS_CSV)\n",
    "gold_transcription_df[\"filename\"] = gold_transcription_df[\"audio_filename\"].str.lower()\n",
    "gold_transcription_df[\"ipa_transcription\"] = gold_transcription_df[\"ipa_transcription\"].str.replace(\"ɝ\", \"ɹ̩\")\n",
    "vowel_counts = Counter()\n",
    "for vowel in TIMIT_VOWELS + TIMIT_DIPHTHONGS:\n",
    "    vowel_counts[vowel] += gold_transcription_df[\"ipa_transcription\"].apply(lambda x: x.split().count(vowel)).sum()\n",
    "\n",
    "vowel_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5236dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple bar chart of vowel counts\n",
    "plot_vowels, plot_counts = zip(*vowel_counts.most_common())\n",
    "g = sns.barplot(y=plot_vowels, x=plot_counts, palette=\"colorblind\")\n",
    "g.set_xlim(0, 13500)\n",
    "g.set_xlabel(\"count\")\n",
    "g.set(title=\"Counts of TIMIT Vowel Occurrences\")\n",
    "for bar in g.containers:\n",
    "    g.bar_label(bar, fontsize='small')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d32bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = \"***\"\n",
    "def tally_edit_distance_errors(references, predictions):\n",
    "    \"\"\"Counts up edit distances from lists of already tokenized references and predictions.\"\"\"\n",
    "    subs = Counter()\n",
    "    insertions = Counter()\n",
    "    deletions = Counter()\n",
    "    for ref_tokens, pred_tokens in zip(references, predictions):\n",
    "        aligned_pairs = kaldialign.align(ref_tokens, pred_tokens, EPS)\n",
    "\n",
    "        for r, p in aligned_pairs:\n",
    "            if r == EPS:\n",
    "                insertions[p] += 1\n",
    "            elif p == EPS:\n",
    "                deletions[r] += 1\n",
    "            elif r != p:\n",
    "                subs[(r, p)] += 1\n",
    "\n",
    "    return subs, deletions, insertions\n",
    "\n",
    "def diphthong_merge(t1, t2):\n",
    "    \"\"\"For merge detected diphthongs in predicted output when using ipatok.tokenise\"\"\"\n",
    "    if t1+t2 in TIMIT_DIPHTHONGS:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a81f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-do edit distance calculations with better tokenization, specifically turning\n",
    "# on diphthong tokenization\n",
    "our_model_detailed_preds_df = pd.read_csv(DETAILED_PRED_DIR / \"ginic_full_dataset_train_1_wav2vec2-large-xlsr-53-buckeye-ipa_detailed_predictions.csv\").drop(columns=[\"substitutions\", \"insertions\", \"deletions\"])\n",
    "full_edit_distance_analysis_df = pd.merge(gold_transcription_df, our_model_detailed_preds_df, on=\"filename\")\n",
    "\n",
    "full_edit_distance_analysis_df[\"ipa_tokens\"] = full_edit_distance_analysis_df[\"ipa_transcription\"].str.split()\n",
    "full_edit_distance_analysis_df[\"predicted_ipa_tokens\"] = full_edit_distance_analysis_df[\"prediction\"].apply(lambda x: ipatok.tokenise(x, diphthongs=True, merge=diphthong_merge))\n",
    "print(full_edit_distance_analysis_df[\"ipa_tokens\"][:10])\n",
    "print(full_edit_distance_analysis_df[\"predicted_ipa_tokens\"][:10])\n",
    "display(full_edit_distance_analysis_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cade8632",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_counter, del_counter, inserts_counter = tally_edit_distance_errors(full_edit_distance_analysis_df[\"ipa_tokens\"], full_edit_distance_analysis_df[\"predicted_ipa_tokens\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb465936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get subs and deletions in good format for analysis\n",
    "detailed_error_counts = defaultdict(Counter)\n",
    "subs_counts = Counter()\n",
    "for (sub_tuple, count) in sub_counter.items():\n",
    "    subs_counts[sub_tuple[0]] += count\n",
    "    detailed_error_counts[sub_tuple[0]][sub_tuple[1]] += count\n",
    "\n",
    "print(\"Substitution Counts:\", subs_counts)\n",
    "\n",
    "for (deleted, count) in del_counter.items():\n",
    "    detailed_error_counts[deleted][\"<deleted>\"] += count\n",
    "\n",
    "print(\"Detailed Error Counts:\", detailed_error_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc57b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute vowel error rates\n",
    "vowel_error_rates = {}\n",
    "for v in TIMIT_VOWELS + TIMIT_DIPHTHONGS:\n",
    "    subs_count = subs_counts[v]\n",
    "    dels_count = del_counter[v]\n",
    "    ver = (subs_count + dels_count)/ (vowel_counts[v])\n",
    "    vowel_error_rates[v] = ver\n",
    "\n",
    "ver_df = pd.DataFrame(vowel_error_rates.items(), columns=[\"Vowel\", \"Vowel Error Rate\"]). sort_values(by=\"Vowel Error Rate\", ascending=False)\n",
    "error_ordering = ver_df[ver_df[\"Vowel Error Rate\"] > 0][\"Vowel\"].tolist()\n",
    "print(\"In descending frequency of errors:\", error_ordering)\n",
    "\n",
    "display(ver_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 6))\n",
    "sns.heatmap(\n",
    "    ver_df.sort_values(by=\"Vowel Error Rate\", ascending=False).set_index(\"Vowel\"),\n",
    "    cmap=\"rainbow\",\n",
    "    # cmap=\"spring_r\",\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    yticklabels=True,\n",
    "    # linewidths=1,\n",
    ")\n",
    "plt.title(\"AutoIPA TIMIT Vowel Error Rates\\n(Descending worst to best)\")\n",
    "# plt.xticks(rotation=90)\n",
    "plt.yticks(rotation=0)\n",
    "plt.ylabel(\"\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_vowels = ver_df[ver_df[\"Vowel Error Rate\"] > 0.0][\"Vowel\"].tolist()\n",
    "print(interesting_vowels)\n",
    "\n",
    "\n",
    "interesting_errors = []\n",
    "for v in interesting_vowels:\n",
    "    for error, count in detailed_error_counts[v].items():\n",
    "        interesting_errors.append((v, error, count))\n",
    "\n",
    "interesting_errors_df = pd.DataFrame(interesting_errors, columns=[\"Vowel\", \"Error\", \"Count\"])\n",
    "interesting_errors_df[\"Ratio of Vowel's Errors\"] = interesting_errors_df.groupby(\"Vowel\", group_keys=False)[\"Count\"].apply(lambda x: x / x.sum())\n",
    "display(interesting_errors_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab top ten errors for each vowel\n",
    "top_errors_df = interesting_errors_df.groupby(\"Vowel\").apply(lambda x: x.nlargest(5, \"Count\")).reset_index(drop=True)\n",
    "top_errors_df[\"Vowel\"] = pd.Categorical(top_errors_df[\"Vowel\"], categories=error_ordering, ordered=True)\n",
    "top_errors_df = top_errors_df.sort_values(by=[\"Vowel\", \"Count\"], ascending=[True, False])\n",
    "display(top_errors_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150e7975",
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_errors = [\"ɨ\", \"ʉ\", \"ə̥\", \"ə\", \"ɚ\"]\n",
    "\n",
    "convention_errors_df = top_errors_df[top_errors_df[\"Vowel\"].isin(convention_errors)]\n",
    "convention_errors_df[\"Vowel\"] = pd.Categorical(convention_errors_df[\"Vowel\"], categories=convention_errors, ordered=True)\n",
    "g = sns.FacetGrid(convention_errors_df, col=\"Vowel\", col_wrap=3, sharey=False, xlim=(0, 1), aspect=1.25)\n",
    "g.map_dataframe(sns.barplot, x=\"Ratio of Vowel's Errors\", y=\"Error\", orient=\"h\")\n",
    "g.set_titles(col_template=\"{col_name}\")\n",
    "g.set_ylabels(\"Error or\\nSubstitution\")\n",
    "g.set_xlabels(\"As ratio of total errors\\naffecting the vowel\")\n",
    "g.fig.suptitle(\"Top 5 errors for vowels Wav2IPA always incorrectly transcribes\", fontsize=24, y=1.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e238e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_convention_errors = [v for v in interesting_vowels if v not in convention_errors]\n",
    "print(not_convention_errors)\n",
    "not_convention_errors_df = top_errors_df[top_errors_df[\"Vowel\"].isin(not_convention_errors)]\n",
    "not_convention_errors_df[\"Vowel\"] = pd.Categorical(\n",
    "not_convention_errors_df[\"Vowel\"], categories=not_convention_errors, ordered=True\n",
    ")\n",
    "display(not_convention_errors_df.head(20))\n",
    "\n",
    "g = sns.FacetGrid(not_convention_errors_df, col=\"Vowel\", col_wrap=5, sharey=False, aspect=1.25, xlim=(0, 1))\n",
    "g.map_dataframe(sns.barplot, x=\"Ratio of Vowel's Errors\", y=\"Error\", orient=\"h\")\n",
    "g.set_titles(col_template=\"{col_name}\", fontsize=20)\n",
    "g.set_ylabels(\"Error or\\nSubstitution\")\n",
    "g.set_xlabels(\"As ratio of total errors\\naffecting the vowel\")\n",
    "g.fig.suptitle(\"Remaining TIMIT Vowels: Top 5 Wav2IPA Errors for each vowel\", fontsize=24, y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbb9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multipa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
